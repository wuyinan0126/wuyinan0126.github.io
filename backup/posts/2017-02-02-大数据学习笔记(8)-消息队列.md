---
title:  "<font color='red'>[原创]</font> 大数据学习笔记(8)-消息队列"
date:   2017-02-02 00:00:00
categories: [原创,大数据]
tags: [原创,大数据]
---

*以Kafka为例学习消息队列的相关知识，并在集群上搭建Kafka。包括介绍消息队列、Kafka整体架构等*

## 消息队列
---

消息队列是在消息传输过程中保存消息的容器或中间件，其主要目的是提供消息路由并保障消息的可靠传递。

常见的消息中间件包括ActiveMQ、ZeroMQ、RabbitMQ和Kafka等。一般消息中间件支持两种模式的队列：

1. 消息队列模式
	
	即**消息生产者将消息存入队列，消息消费者从队列消费消息**

2. Pub-Sub模式

	消息生产者将消息**发布到指定主题的队列中**，而消费者**订阅指定主题的队列消息**，当订阅的主题有新消息时，消费者可以通过拉取(Pull)或中间件的推送(Push)消费消息

---

## Kafka
---

Kafka是LinkedIn开源的采用Pub-Sub模式的分布式消息系统，最初被设计作为Log收集工具，因其具有极高吞吐量、低延迟、可扩展、高可用以及能够对消息队列进行持久化保存（不是将全部消息保存在内存中传递），因此应用场景较多，比如作为通用的消息系统、消息实时收集、以及流式计算系统的底层构建等。

---

### 整体架构
---

Kafka架构主要由消息生产者（Producer）、代理服务器（Broker）、消息消费者（Consumer）构成

* Producer：生产者生产指定主题（Topic）的消息并传入代理服务器集群
* Broker：代理服务器集群在磁盘存储维护各种Topic的消息队列
* Consumer：消费者根据自己订阅的Topic从代理服务器集群中**拉取（Pull）**新消息并对其进行处理，每个consumer属于一个consumer group，每个group中可以有多个consumer。发送到Topic的消息，只会被订阅此Topic的每个group中的一个consumer消费

	采用pull方式的好处是消费者可以自主控制消费速率，避免消费者因处理速度跟不上生产者而导致消息大量积压

* Topic：即在内部对应某个名字的消息队列，比如用户访问网站的行为可以分为登录、搜索等不同Topic。Kafka支持对Topic进行数据分片（Partition）
* Partition：每个数据分片是有序的、不可更改的尾部追加的消息队列。队列内的每个消息被分配给该数据分片内唯一的Offset。用户可以根据需求，如用户UID进行哈希分配，使得同一用户的数据会放入相同Partion中
	
	对于某个Partition，在系统中是一系列被切割成固定大小的文件，新消息被追加到最后一个文件的尾部，同时在内存中维护每个文件首个消息的Offset组成的有序数组作为索引，其内容指向外部文件。消费者读取某个消息时，需要指定消息对应的Offset及读取的内容大小

因为Kafka的消息是存储在文件中的，因此天然具有持久化的能力

* 性能优化：Kafka是一个基于文件系统的消息系统，能够高效处理大批量消息的一个重要原因就是尽可能避免随机读写，尽可能转换为顺序读写，即连续读写整块数据，如Log文件尾部追加写

消费者可以通过变换Offset来从对应数据分片读取过期的消息，因此满足“至少送达一次”的语义

Kafka将消费者目前读取到队列中的哪个信息这个信息交由消费者各自保管，并将很多其他管理信息都存放在Zookeeper而非代理服务器中，这样代理服务器完全成为无状态的，无需记载任何状态信息，增强了消息系统的容错和扩展性

---

## 安装Kafka（版本0.10.2.0）
---

### 准备工作 
---

1. 按照[大数据学习笔记(5)-分布式协调系统](https://wuyinan0126.github.io/2017/大数据学习笔记(5)-分布式协调系统/)安装好Zookeeper

2. 下载Kafka二进制包，解压至/opt/:

		$ ansible zookeeper -m copy -a 'src=/home/cat/cat/kafka_2.11-0.10.2.0.tgz dest=/home/cat/cat/'
		$ ansible zookeeper -a 'tar -zxvf /home/cat/cat/kafka_2.11-0.10.2.0.tgz -C /opt/'
		$ ansible zookeeper -a 'mv /opt/kafka_2.11-0.10.2.0 /opt/kafka-0.10.2.0'

3. 创建kafka日志目录:

		$ ansible zookeeper -a 'mkdir /opt/kafka-0.10.2.0/logs'

---

### 配置Kafka
---

1. conf/server.properties

		##### 需要配置 #####
		# 节点唯一标识，类似zookeeper的myid
		broker.id=0  
		host.name=10.2.2.141
		# 消息存放的目录，这个目录可以配置为逗号分割的表达式，num.io.threads要大于这个目录的个数
		log.dirs=/opt/kafka-0.10.2.0/logs/
		# 设置zookeeper的连接端口
		zookeeper.connect=10.2.2.141:2181,10.2.2.142:2181,10.2.2.143:2181 

		##### 使用默认 #####
		# kafka对外提供服务的端口
		port=9092 
		# borker进行网络处理的线程数
		num.network.threads=3 
		# borker进行I/O处理的线程数
		num.io.threads=8 
		# 发送缓冲区buffer大小
		socket.send.buffer.bytes=102400 
		# kafka接收缓冲区大小，当数据到达一定大小后再序列化到磁盘
		socket.receive.buffer.bytes=102400 
		# 这个参数是向kafka请求消息或者向kafka发送消息的请求最大数，这个值不能超过java的堆栈大小
		socket.request.max.bytes=104857600 
		# 默认的分区数，一个topic默认1个分区数
		num.partitions=1 
		# 默认消息的最大持久化时间，168小时，7天
		log.retention.hours=168 
		# 消息保存的最大值5M
		message.max.byte=5242880  
		# kafka保存消息的副本数，如果一个副本失效了，另一个还可以继续提供服务
		default.replication.factor=2  
		# 取消息的最大直接数
		replica.fetch.max.bytes=5242880 
		# 因为kafka的消息是以追加的形式落地到文件，当超过这个值的时候，kafka会新起一个文件
		log.segment.bytes=1073741824 
		# 每隔300秒去检查上面配置的log失效时间（log.retention.hours=168），到目录查看是否有过期的消息如果有，删除
		log.retention.check.interval.ms=300000 
		# 是否启用log压缩，一般不用启用，启用的话可以提高性能
		log.cleaner.enable=false 
		
2. 分发配置文件

		$ ansible zookeeper -m copy -a 'src=/opt/kafka-0.10.2.0/config/ dest=/opt/kafka-0.10.2.0/config/'	

3. 登入每台kafka节点，修改broker.id和host.name

---

### 启动Kafka
---

1. 在每个kafka节点：

		$ kafka-server-start.sh -daemon /opt/kafka-0.10.2.0/config/server.properties

### 测试Kafka
---

1. 在主节点查看

		$ zkCli.sh
		[zk: localhost:2181(CONNECTED) 0] ls /
		[cluster, controller_epoch, controller, brokers, zookeeper, hadoop-ha, admin, isr_change_notification, consumers, config]

2. [可选]创建topic，一个副本，一个分区

		$ kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test

3. [可选]查看topic

		$ kafka-topics.sh --list --zookeeper localhost:2181

4. [可选]使用kafka-console-producer在控制台发送消息

		$ kafka-console-producer.sh --broker-list localhost:9092 --topic test

5. [可选]使用kafka-console-consumer在控制台接受消息
		
		$ kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning

6. [可选]如果在producer console输入一条消息，能从consumer console看到这条消息就代表安装是成功

---