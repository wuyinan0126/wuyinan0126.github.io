---
title:  "<font color='red'>[原创]</font> 大数据学习笔记(3)-Spark"
date:   2016-12-04 00:00:00
categories: [原创,大数据]
tags: [原创,大数据]
---

**

## 安装Scala（版本2.11.8）
---
1. 下载Scala二进制包，解压至/opt/:

		$ tar -zxvf ./scala-2.11.8.tgz -C /opt/

## 安装Spark（版本2.1.0）
---

#### 准备工作 
1. 下载Spark二进制包（或源码自行编译），解压至/opt/:

		$ tar -zxvf ./spark-2.1.0-bin-hadoop2.7 -C /opt/
		$ mv /opt/spark-2.1.0-bin-hadoop2.7 /opt/spark-2.1.0

#### 配置Spark
1. conf/spark-env.sh:

		export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home
		export HADOOP_HOME=/opt/hadoop-2.7.3
		export SCALA_HOME=/opt/scala-2.11.8	
		
		export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
		export HDFS_CONF_DIR=$HADOOP_HOME/etc/hadoop
		export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop

		# spark.history.retainedApplications仅显示最近50个应用
		export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=file:/tmp/spark-events"

2. conf/slaves:

		localhost

3. conf/spark-defaults.conf:
		
		# 对于Spark Job启用log event配置，是否记录Spark事件，用于应用程序在完成后重构webUI
		spark.eventLog.enabled           true
		spark.eventLog.dir               file:/tmp/spark-events
		spark.eventLog.compress          true


#### 启动Spark	
1. 启动Hadoop
		
		$ hadoop-daemon.sh start namenode
		$ hadoop-daemon.sh start datanode
		$ start-yarn.sh

2. 启动Spark，用jps查看，启动的进程应包括Master、Worker:

		$ /opt/spark-2.1.0/sbin/start-all.sh

3. 先创建history默认存放位置，启动HistoryServer，启动的进程应包括HistoryServer：

		$ mkdir /tmp/spark-events/
		$ /opt/spark-2.1.0/sbin/start-history-server.sh

4. 测试，运行SparkPi例子:

		$ cd /opt/spark-2.1.0
		$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi \
		--master yarn-cluster \
		--num-executors 3 \
		--driver-memory 2g \
		--executor-memory 1g \
		--executor-cores 1 \
		--queue thequeue \
		examples/jars/spark-examples_2.11-2.1.0.jar \
		10

5. 查看结果:

		$ yarn logs -applicationId ${applicationId} | grep 'Pi'

6. 在界面[http://127.0.0.1:18080/](http://127.0.0.1:18080/)查看历史


