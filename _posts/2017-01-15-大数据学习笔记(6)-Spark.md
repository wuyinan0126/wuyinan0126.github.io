---
title:  "<font color='red'>[原创]</font> 大数据学习笔记(6)-Spark"
date:   2017-01-15 00:00:00
categories: [原创,大数据]
tags: [原创,大数据]
---

*深入学习Spark相关知识，并在Yarn集群搭建Spark。包括通过日志分析Spark on Yarn-cluster运行机制*

## Spark
---

### Spark on Yarn-cluster运行机制
---

在Spark on Yarn-cluster模式中，Spark应用的Driver即为Yarn中的ApplicationMaster(实际也是一个Container)，Spark的Executor即为Yarn中的Container，其中Driver负责资源的申请和job的调度，Executor负责Task的具体执行

运行参数：

	spark-submit --class logs.Main \
	 --master yarn-cluster \		// Spark on Yarn-cluster
	 --num-executors 19 \			// 19个Executors+1个AM(Driver)
	 --executor-memory 5g \			// 每个Executors堆内存
	 --executor-cores 2 \			// 每个Executors虚拟CPU核数
	 --driver-memory 5g \			//  Driver堆内存
	./Test-assembly-1.0.jar

1. Spark Yarn的client提交应用(Application)给Yarn的ResourceManager

		// 连接到RM
		17/03/31 18:14:19 INFO RMProxy: Connecting to ResourceManager at /10.2.2.141:8032

		// 向集群请求提交一个应用
		17/03/31 18:14:19 INFO Client: Requesting a new application from cluster with 10 NodeManagers
		
		// 检查请求的资源是否超过设定值
		17/03/31 18:14:19 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)
		
		// 分配一个ApplicationMaster，包括堆内存5120M和非堆内存512M，共5632M
		17/03/31 18:14:19 INFO Client: Will allocate AM container, with 5632 MB memory including 512 MB overhead

2. RM向NodeManager申请Container，用于运行ApplicationMaster(Spark的Driver)，并将应用的jar文件上传到HDFS

		// 申请Container，用于运行ApplicationMaster
		17/03/31 18:14:19 INFO Client: Setting up container launch context for our AM
		17/03/31 18:14:19 INFO Client: Setting up the launch environment for our AM container
		17/03/31 18:14:19 INFO Client: Preparing resources for our AM container

		// 将应用的jar文件上传到HDFS
		17/03/31 18:14:22 INFO Client: Uploading resource file:/tmp/spark-0222f8da-3cf4-4ff1-9109-c4877293fc5b/__spark_libs__3620896466196856366.zip -> hdfs://cats/user/wyn/.sparkStaging/application_1490945694024_0003/__spark_libs__3620896466196856366.zip

3. NM启动AM，在Container中初始化SparkContext，其中比较重要的是创建TaskScheduler和DAGScheduler。在Spark on Yarn-cluster模式中，TaskScheduler将选择YarnClusterScheduler和YarnClusterSchedulerBackend

	* YarnClusterScheduler用于根据每个Executor的资源剩余情况分配合适的Task，并维护一个任务队列，根据FIFO或Fair策略，调度任务
	* YarnClusterSchedulerBackend用于维护Executor相关信息(包括Executor的地址、通信端口、主机、资源情况)
	* DAGScheduler用于根据Job构建基于Stage的DAG(有向无环图)，并提交Stage给TaskScheduler

			17/03/31 18:14:29 INFO ApplicationMaster: Preparing Local resources
			
			// 初始化SparkContext
			17/03/31 18:14:30 INFO ApplicationMaster: Waiting for spark context initialization...
			17/03/31 18:14:30 INFO SparkContext: Running Spark version 2.1.0
			
			// 创建YarnClusterScheduler
			17/03/31 18:14:31 INFO YarnClusterScheduler: Created YarnClusterScheduler
			
			// AM中注册一个SchedulerBackend，用于Executor与其通信
			17/03/31 18:14:31 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@10.2.2.150:45262)

4. AM向RM申请资源，申请到相应资源后，AM中的YarnClusterScheduler通过RPC协议让NM启动相应的Executor

		// 申请资源，包括堆内存5120M和非堆内存512M，共5632M
		17/03/31 18:14:31 INFO YarnAllocator: Will request 19 executor container(s), each with 2 core(s) and 5632 MB memory (including 512 MB of overhead)
		17/03/31 18:14:31 INFO YarnAllocator: Submitted 19 unlocalized container requests.
		
		// 启动executor
		17/03/31 18:14:31 INFO YarnAllocator: Launching container container_1490945694024_0003_01_000002 on host cat5
		17/03/31 18:14:31 INFO YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
		...

5. NM启动Executor(container)，在container中创建ExecutorBackend，用于与AM的SchedulerBackend通信，并向AM注册并申请Task

		// 每个executor中注册一个ExecutorBackend，用于与AM通信
		17/03/31 18:14:33 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (10.2.2.150:34052) with ID 2
		17/03/31 18:14:34 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (10.2.2.144:39920) with ID 4
		...

6. 开始执行Job，DAGScheduler根据Job构建基于Stage的DAG，并提交Stage(也就是TaskSet)给TaskScheduler，TaskScheduler向Executor分配Task，由ExecutorBackend执行，并向AM汇报运行的状态和进度

		// 开始执行Job
		17/03/31 18:14:50 INFO SparkContext: Starting job: sortBy at Main.scala:33
		
		// DAGScheduler根据Job构建基于Stage的DAG，并提交Stage(也就是TaskSet)给TaskScheduler
		17/03/31 18:14:51 INFO DAGScheduler: Submitting 23137 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at groupBy at Main.scala:31)
		
		// TaskScheduler将Task分配给Executor
		17/03/31 18:14:51 INFO YarnClusterScheduler: Adding task set 0.0 with 23137 tasks
		
		// 分配给的Executor的具体信息
		17/03/31 18:14:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, cat12, executor 12, partition 0, NODE_LOCAL, 6051 bytes)
		17/03/31 18:14:51 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 1, cat8, executor 16, partition 18, NODE_LOCAL, 6051 bytes)
		...
		17/03/31 18:14:52 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 32) in 1413 ms on cat5 (executor 11) (1/23137)

7. 应用程序运行完成后，AM下令关闭所有Executor，并向RM申请注销并关闭自己
	
		// AM通知应用运行完成
		17/03/31 18:18:50 INFO ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0
		17/03/31 18:18:50 INFO SparkContext: Invoking stop() from shutdown hook
		
		// AM下令关闭所有Executor
		17/03/31 18:18:53 INFO YarnAllocator: Driver requested a total number of 0 executor(s).
		17/03/31 18:18:53 INFO YarnClusterSchedulerBackend: Shutting down all executors
		17/03/31 18:18:53 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
		
		// AM关闭自己
		17/03/31 18:18:53 INFO SparkContext: Successfully stopped SparkContext
		17/03/31 18:18:53 INFO ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED
		
		// Executor收到来自Driver的关闭命令
		17/03/31 18:18:53 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
		
		// Executor关闭
		17/03/31 18:18:53 INFO CoarseGrainedExecutorBackend: Driver from 10.2.2.150:45262 disconnected during shutdown
		17/03/31 18:18:53 INFO ShutdownHookManager: Shutdown hook called

---

## 安装Spark（版本2.1.0）
---

### 准备工作 
---

1. 按照[大数据学习笔记(2)-Hadoop](https://wuyinan0126.github.io/2016/大数据学习笔记(2)-Hadoop/)做好部署前提准备

2. 下载Scala（版本2.11.8）二进制包，并分发到所有节点，并解压至/opt/:

		$ ansible all -m copy -a 'src=/home/cat/cat/scala-2.11.8.tgz dest=/home/cat/cat/'
		$ ansible all -a 'tar -zxvf /home/cat/cat/scala-2.11.8.tgz -C /opt/'

3. 下载Spark二进制包（或源码自行编译），并分发到所有节点，并解压至/opt/:

		$ ansible all -m copy -a 'src=/home/cat/cat/spark-2.1.0-bin-hadoop2.7.tgz dest=/home/cat/cat/'
		$ ansible all -a 'tar -zxvf /home/cat/cat/spark-2.1.0-bin-hadoop2.7.tgz -C /opt/'
		$ ansible all -a 'mv /opt/spark-2.1.0-bin-hadoop2.7 /opt/spark-2.1.0'

3. 创建history默认存放位置:

		$ hadoop fs -mkdir /tmp
		$ hadoop fs -mkdir /tmp/spark-history
		$ hadoop fs -chmod -R 777 /tmp

---

### 配置Spark
---

1. conf/spark-env.sh:

		export JAVA_HOME=/opt/jdk1.8.0_121
		export HADOOP_HOME=/opt/hadoop-2.7.3
		export SCALA_HOME=/opt/scala-2.11.8	
		
		export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
		export HDFS_CONF_DIR=$HADOOP_HOME/etc/hadoop
		export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop

		# spark.history.retainedApplications仅显示最近10个应用
		export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs:///tmp/spark-history"

2. conf/slaves:

		10.2.2.14[3:12]

3. conf/spark-defaults.conf:
		
		# 对于Spark Job启用log event配置，是否记录Spark事件，用于应用程序在完成后重构webUI
		spark.eventLog.enabled           true
		spark.eventLog.dir hdfs:///tmp/spark-history
		spark.eventLog.compress          true

		spark.yarn.historyServer.address 10.2.2.141:18080
		spark.history.ui.port 18080
		spark.history.fs.logDirectory hdfs:///tmp/spark-history
		spark.history.provider org.apache.spark.deploy.history.FsHistoryProvider

4. 分发Spark配置文件

		$ ansible slave -m copy -a 'src=/opt/spark-2.1.0/conf/ dest=/opt/spark-2.1.0/conf/'

5. 使用spark_shuffle，修改$HADOOP_HOME/etc/hadoop/yarn-site.xml:

		<property>
		  <name>yarn.nodemanager.aux-services</name>
		  <value>mapreduce_shuffle,spark_shuffle</value>
		</property>
		<property>
		  <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
		  <value>org.apache.spark.network.yarn.YarnShuffleService</value>
		</property>

6. 配置了5之后需要将spark的yarn目录里的spark-2.1.0-yarn-shuffle.jar加入yarn classpath:
		
		$ yarn classpath
		# 将spark-network-shuffle_2.11-2.1.0.jar放入上述输出的某一路径中，如/opt/hadoop-2.7.3/share/hadoop/yarn/lib/下
		$ ansible all -a 'cp /opt/spark-2.1.0/yarn/spark-2.1.0-yarn-shuffle.jar /opt/hadoop-2.7.3/share/hadoop/yarn/'

7. 配置了5之后需要修改$HADOOP_HOME/etc/hadoop/yarn-env.sh:

		# 因为使用了spark_shuffle，因此需要提高YARN_HEAPSIZE，避免在shuffle过程中的garbage collection问题
		YARN_HEAPSIZE=5000

8. 分发Hadoop配置文件

		$ ansible slave -m copy -a 'src=/opt/hadoop-2.7.3/etc/hadoop/ dest=/opt/hadoop-2.7.3/etc/hadoop/'


---

### 启动Spark
---

1. 启动Spark的所有进程，包括Master、Worker:

		$ /opt/spark-2.1.0/sbin/start-all.sh

2. (在主节点)启动HistoryServer，启动的进程应包括HistoryServer：

		$ /opt/spark-2.1.0/sbin/start-history-server.sh

### 测试Spark
---

1. 使用jps查看各节点起的进程

		$ ansible all -a '/opt/jdk1.8.0_121/bin/jps'

		cat1: Master, HistoryServer, QuorumPeerMain, DFSZKFailoverController, NameNode, ResourceManager
		cat2: QuorumPeerMain, DFSZKFailoverController, NameNode
		cat3: QuorumPeerMain
		cat4-12: Worker, JournalNode, DataNode, NodeManager

2. 测试，运行SparkPi例子:

		$ /opt/spark-2.1.0/bin/spark-submit --class org.apache.spark.examples.SparkPi \
		--master yarn-cluster \
		--num-executors 3 \
		--driver-memory 2g \
		--executor-memory 1g \
		--executor-cores 1 \
		--queue thequeue \
		/opt/spark-2.1.0/examples/jars/spark-examples_2.11-2.1.0.jar \
		10

3. 查看结果:

		$ yarn logs -applicationId ${applicationId} | grep 'Pi'

4. 在界面[http://10.2.2.141:18080/](http://10.2.2.141:18080/)查看历史

---
