---
title:  "<font color='red'>[原创]</font> 大数据学习笔记(6)-Spark"
date:   2017-01-15 00:00:00
categories: [原创,大数据]
tags: [原创,大数据]
---

**

## 安装Spark（版本2.1.0）
---

### 准备工作 
---

1. 按照[大数据学习笔记(2)-Hadoop](https://wuyinan0126.github.io/2016/大数据学习笔记(2)-Hadoop/)做好部署前提准备

2. 下载Scala（版本2.11.8）二进制包，并分发到所有节点，并解压至/opt/:

		$ ansible all -m copy -a 'src=/home/cat/cat/scala-2.11.8.tgz dest=/home/cat/cat/'
		$ ansible all -a 'tar -zxvf /home/cat/cat/scala-2.11.8.tgz -C /opt/'

3. 下载Spark二进制包（或源码自行编译），并分发到所有节点，并解压至/opt/:

		$ ansible all -m copy -a 'src=/home/cat/cat/spark-2.1.0-bin-hadoop2.7.tgz dest=/home/cat/cat/'
		$ ansible all -a 'tar -zxvf /home/cat/cat/spark-2.1.0-bin-hadoop2.7.tgz -C /opt/'
		$ ansible all -a 'mv /opt/spark-2.1.0-bin-hadoop2.7 /opt/spark-2.1.0'

3. 创建history默认存放位置:

		$ hadoop fs -mkdir /tmp
		$ hadoop fs -mkdir /tmp/spark-history
		$ hadoop fs -chmod -R 777 /tmp

---

### 配置Spark
---

1. conf/spark-env.sh:

		export JAVA_HOME=/opt/jdk1.8.0_121
		export HADOOP_HOME=/opt/hadoop-2.7.3
		export SCALA_HOME=/opt/scala-2.11.8	
		
		export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
		export HDFS_CONF_DIR=$HADOOP_HOME/etc/hadoop
		export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop

		# spark.history.retainedApplications仅显示最近10个应用
		export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs:///tmp/spark-history"

2. conf/slaves:

		10.2.2.14[3:12]

3. conf/spark-defaults.conf:
		
		# 对于Spark Job启用log event配置，是否记录Spark事件，用于应用程序在完成后重构webUI
		spark.eventLog.enabled           true
		spark.eventLog.dir hdfs:///tmp/spark-history
		spark.eventLog.compress          true

		spark.yarn.historyServer.address 10.2.2.141:18080
		spark.history.ui.port 18080
		spark.history.fs.logDirectory hdfs:///tmp/spark-history
		spark.history.provider org.apache.spark.deploy.history.FsHistoryProvider

4. 分发Spark配置文件

		$ ansible slave -m copy -a 'src=/opt/spark-2.1.0/conf/ dest=/opt/spark-2.1.0/conf/'

---

### 启动Spark
---

1. 启动Spark的所有进程，包括Master、Worker:

		$ /opt/spark-2.1.0/sbin/start-all.sh

2. (在主节点)启动HistoryServer，启动的进程应包括HistoryServer：

		$ /opt/spark-2.1.0/sbin/start-history-server.sh

### 测试Spark
---

1. 使用jps查看各节点起的进程

		$ ansible all -a '/opt/jdk1.8.0_121/bin/jps'

		cat1: Master, HistoryServer, QuorumPeerMain, DFSZKFailoverController, NameNode, ResourceManager
		cat2: QuorumPeerMain, DFSZKFailoverController, NameNode
		cat3: QuorumPeerMain
		cat4-12: Worker, JournalNode, DataNode, NodeManager

2. 测试，运行SparkPi例子:

		$ /opt/spark-2.1.0/bin/spark-submit --class org.apache.spark.examples.SparkPi \
		--master yarn-cluster \
		--num-executors 3 \
		--driver-memory 2g \
		--executor-memory 1g \
		--executor-cores 1 \
		--queue thequeue \
		/opt/spark-2.1.0/examples/jars/spark-examples_2.11-2.1.0.jar \
		10

3. 查看结果:

		$ yarn logs -applicationId ${applicationId} | grep 'Pi'

4. 在界面[http://10.2.2.141:18080/](http://10.2.2.141:18080/)查看历史

---

## 下一步：[安装Hive]((https://wuyinan0126.github.io/2017/大数据学习笔记(7)-数据仓库/))

---
