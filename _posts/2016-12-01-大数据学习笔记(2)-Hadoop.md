---
title:  "<font color='red'>[原创]</font> 大数据学习笔记(2)-Hadoop"
date:   2016-12-01 00:00:00
categories: [原创,大数据]
tags: [原创,大数据]
---

*深入学习Hadoop相关知识，并在集群上搭建Hadoop HA平台。包括介绍MapReduce计算框架、HDFS文件系统、YARN资源调度与管理系统、MapReduce和Yarn的参数配置等*

## Hadoop
---

### MapReduce
---

大数据计算中最常见的一种计算方式是**批处理**。2004年Google发表了MapReduce计算范型和框架论文，这是一种构建在大规模PC之上的批处理计算框架，也是一种分布式计算模型，获得了极为广泛的应用。

MapReduce和传统的并行数据库系统(MPP)相比，更适合**非结构化数据的ETL处理**(将数据从来源端经过抽取(extract)、转换(transform)、加载(load)至目的端的过程)，且其更具有**扩展性和容错性**，但单机处理效率低。

MapReduce计算提供了简洁的编程接口，**输入和输出均是Key/Value键值对**，只需根据业务逻辑实现Map和Reduce函数即可完成大规模数据的并行批处理任务。

* Map函数以Key/Value作为输入，经逻辑计算产生若干仍以Key/Value形式表达的中间数据。计算框架自动将中间数据中具有相同Key值的记录聚在一起，传递给Reduce函数作为输入
* Reduce函数以Map阶段传递过来的某个Key值及其对应的若干Value值作为输入，对这些Value进行逻辑计算，生成Key/Value即计算结果

以上为MapReduce的简单介绍，更为详细的MapReduce计算模型学习笔记请参看我的另一篇博文：[大数据学习笔记(3)-计算模型](https://wuyinan0126.github.io/2016/大数据学习笔记(3)-计算模型/)

---

### HDFS
---

HDFS的全称是"Hadoop Distributed File System"，是Hadoop中的大规模分布式文件系统。它最初是Yahoo模仿Goole的GFS(Google File System)开发的开源系统，因此在整体架构上和GFS大致相同。与GFS一样，HDFS适合存储**大文件**并为之提供高吞吐量的**顺序读写**访问，不太适合大量随机读的应用场景，也不适合储存大量的小文件的应用场景。

在Hadoop1.x中，HDFS可以被看作简化版的开源GFS系统，因为其在实现时绕开了一些GFS的复杂方案而采用简化方案，因此限制了性能。在这个系列的版本中，存在的问题包括：单点失效和水平扩展不佳。单点失效会导致整个集群不可用，而水平扩展不佳会导致整个文件系统管理文件数目容易达到上限，所以集群规模达到一定程度就无法扩展。

在Hadoop2.x中，提出了高可用方案(High Availability, HA)和NameNode联盟(NameNode Federation)。其中高可用是为了解决单点失效问题，而NameNode联盟是为了解决整个系统的水平扩展问题。

以上为HDFS的简单介绍，更为详细的HDFS学习笔记请参看我的另一篇博文：[大数据学习笔记(4)-分布式文件系统](https://wuyinan0126.github.io/2016/大数据学习笔记(4)-分布式文件系统/)

---

### YARN
---

YARN的全称是"Yet Another Resource Negotiator"，是一个独立的**资源调度与管理系统**。

在Hadoop1.x中，所有任务的**资源管理**以及**生命周期管理**都由全局唯一的JobTracker负责，造成其功能繁复，限制了系统的可扩展性。同时JobTracker还存在单点失效的问题，一旦JobTracker故障，整个Hadoop集群将崩溃。

在Hadoop2.x中，将**资源管理**与**任务生命周期管理**功能分离，由ResourceManager(RM)负责资源管理，由ApplicationMaster(AM)负责任务所需资源申请管理与任务生命周期管理。这样的好处除了**增强系统的可扩展性**，**解决JobTracker单点故障问题**，还**可以部署除MapReduce计算框架外的其他计算框架，共享底层硬件资源**。

以上为YARN的简单介绍，更为详细的YARN学习笔记请参看我的另一篇博文：[大数据学习笔记()-资源调度和管理系统](https://wuyinan0126.github.io/2016/大数据学习笔记()-资源调度和管理系统/)

---

### MapReduce和Yarn的参数配置
---

Yarn中资源的最小单位是容器（Container）。容器是一个Yarn的JVM进程，在MapReduce中，AM服务、map和reduce任务都是运行在一个容器中。可以通过[http://{RM_IP}:8088/cluster/scheduler](http://{RM_IP}:8088/cluster/scheduler)来查看正在运行的容器的状态

* 在NM中，可以优化的参数有：

	* yarn.node-manager.resource.memory-mb

	此参数为物理机可用于分配给容器的**物理内存**量，注意需要给物理机留出部分资源给操作系统使用，建议为总物理内存的80%~90%，默认为8192。在不同硬件配置的节点中，该值需要根据物理机实际配置来写

	* yarn.node-manager.resource.vcore

	同理，为物理机可用于分配给容器的虚拟CPU数（我理解为线程数），建议为物理CPU核数。默认为8，如果节点CPU核数不够8个，则需要调减小这个值，YARN不会智能的探测节点的物理CPU总数。在不同硬件配置的节点中，该值需要根据物理机实际配置来写

	* yarn.nodemanager.vmem-pmem-ratio

	此参数是指当任务每使用1MB**物理内存**，最多可使用默认为2.1MB的**虚拟内存**量。在JVM中，使用的内存分为虚拟内存和物理内存。JVM中所有存在内存中的对象都是虚拟内存，但在实际运行中只有一部分是实际加载在物理内存中的

* 在RM中，可以优化的参数有：

	* yarn.scheduler.minimum-allocation-mb

	当应用程序向RM申请容器时，RM按照此**物理内存**量为单位分配给容器，默认为1024。例如，我们设置分配的最小单位为2GB，则RM分配出来的容器内存一定是2G的倍数。假设现在有一个程序向RM申请3.1G的内存，则RM会分配给它一个4GB的容器去执行

	* yarn.scheduler.maximum-allocation-mb

	此参数限定了RM可以分配给一个容器的最大**物理内存**量，默认为8192。假设应用程序向RM申请的资源超过了这个值，RM会直接拒绝这个请求

	* yarn.scheduler.maximum-allocation-vcores

	同理，RM可以分配给一个容器的最多虚拟CPU个数，默认为32

* 在MapReduce中，可以优化的参数有：

	* mapreduce.map.memory.mb
	* mapreduce.reduce.memory.mb

	用于AM向Yarn申请用于map或reduce task的容器时默认使用的**物理内存**值，其值应该在RM中的一个容器可分配的最大最小内存之间，默认为1024。这个值是全局的，因此需要根据某个job的map或reduce task需要使用的内存量，在程序中覆盖这些参数

	* mapreduce.map.java.opts
	* mapreduce.reduce.java.opts

	因为Yarn容器可以理解为一个JVM，这两个值主要是为需要运行JVM程序（java、scala等）准备的，用于设置JVM进程的堆大小。这个值应该比上面的物理内存值小（因为JVM除了堆还有别的对象需要占用内存），一般设置为上面值的0.75倍，如果JVM进程在执行中，堆上的对象申请的内存超过这个值，就会抛出OutOfMemory异常

举个栗子：

* yarn.nodemanager.vmem-pmem-ratio = 2.1
* yarn.scheduler.minimum-allocation-mb = 1024
* mapreduce.map.memory.mb = 1536
* mapreduce.map.java.opts = 1024

此时，由于mapreduce.map.memory.mb=1536，因此AM将向RM申请一个2048M内存的容器，当map task使用的物理内存量大于2048M时或使用的虚拟内存量大于2048*2.1=3225.6M时将被杀死。容器中有1024M内存用作JVM堆内存，其余内存为非堆内存。当java、scala等进程在堆内存中申请的内存超过1024M，将抛出OutOfMemory异常

---

#### 实用工具
---

配置内存参数可以使用[yarn-util.py](https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.6/bk_installing_manually_book/content/determine-hdp-memory-config.html)进行参考

	# 查看每个物理CPU中core的个数(即核数)
	$ cat /proc/cpuinfo| grep "cpu cores"| uniq
	4
	$ python yarn-utils.py -c 4 -m 16 -d 1 -k True
	 Using cores=4 memory=16GB disks=1 hbase=True
	 Profile: cores=4 memory=12288MB reserved=4GB usableMem=12GB disks=1
	 Num Container=3
	 Container Ram=4096MB
	 Used Ram=12GB
	 Unused Ram=4GB
	 yarn.scheduler.minimum-allocation-mb=4096
	 yarn.scheduler.maximum-allocation-mb=12288
	 yarn.nodemanager.resource.memory-mb=12288
	 mapreduce.map.memory.mb=4096
	 mapreduce.map.java.opts=-Xmx3276m
	 mapreduce.reduce.memory.mb=4096
	 mapreduce.reduce.java.opts=-Xmx3276m
	 yarn.app.mapreduce.am.resource.mb=4096
	 yarn.app.mapreduce.am.command-opts=-Xmx3276m
	 mapreduce.task.io.sort.mb=1638

---

## 安装Hadoop HA（版本2.7.3）
---

### 准备工作 
---

1. 按照[大数据学习笔记(1)-Overview](https://wuyinan0126.github.io/2016/大数据学习笔记(1)-Overview/)和[大数据学习笔记(5)-分布式协调系统](https://wuyinan0126.github.io/2017/大数据学习笔记(5)-分布式协调系统/)做好部署前提准备

2. 下载Hadoop二进制包，并分发到所有节点，并解压至/opt/:

		$ ansible all -m copy -a 'src=/home/cat/cat/hadoop-2.7.3.tar.gz dest=/home/cat/cat/'
		$ ansible all -a 'tar -zxvf /home/cat/cat/hadoop-2.7.3.tar.gz -C /opt/'

2. 创建数据文件夹、日志文件夹和myid文件:
		
		$ ansible all -a 'mkdir -p /opt/hadoop-2.7.3/var/name'
		$ ansible all -a 'mkdir -p /opt/hadoop-2.7.3/var/data'
		$ ansible all -a 'mkdir -p /opt/hadoop-2.7.3/var/tmp'
		$ ansible all -a 'mkdir -p /opt/hadoop-2.7.3/var/journal'

---

### 配置Hadoop
---		 

1. etc/hadoop/core-site.xml

		<configuration>
		  <property>      
		    <name>fs.defaultFS</name>      
		    <value>hdfs://cats</value> 
		    <description>默认的HDFS路径，与hdfs-site.xml中的配置相关。当有多个HDFS集群存在时，用此名字指定集群</description> 
		  </property>  
		  <property>      
		    <name>hadoop.tmp.dir</name>     
		    <value>file:///opt/hadoop-2.7.3/var/tmp</value> 
		    <description>NameNode、DataNode、JournalNode等存放数据的公共目录，也可以自己单独指定这三类节点的目录</description>
		  </property>  
		  <property>
		    <name>hadoop.http.staticuser.user</name>
		    <value>cat</value>
		    <description>WEB UI登录用户，默认为Dr.Who，权限较小</description>
		  </property>
		  <property>      
		    <name>ha.zookeeper.quorum</name>      
		    <value>10.2.2.141:2181,10.2.2.142:2181,10.2.2.143:2181</value>
		    <description>这里是ZooKeeper集群的地址和端口。注意，数量一定是奇数，且不少于三个节点</description>
		  </property>
		</configuration>    
  
2. etc/hadoop/hadoop-env.sh

		export JAVA_HOME=/opt/jdk1.8.0_121
		export HADOOP_PID_DIR=/opt/hadoop-2.7.3/var/tmp
		export HADOOP_SECURE_DN_PID_DIR=/opt/hadoop-2.7.3/var/tmp

3. etc/hadoop/hdfs-site.xml

		<configuration>
		  <property>
		    <name>dfs.name.dir</name>
		    <value>file:///opt/hadoop-2.7.3/var/name</value>
		    <description>NameNode存储命名空间和操作日志相关的元数据信息的本地文件系统路径</description>
		  </property>
		  <property>
		    <name>dfs.data.dir</name>
		    <value>file:///opt/hadoop-2.7.3/var/data</value>
		    <description>DataNode节点存储HDFS文件的本地文件系统路径</description>
		  </property>
		  <property>
		    <name>dfs.replication</name>  
		    <value>3</value>
		    <description>指定DataNode存储block的副本数量。默认值是3个，不大于DataNode个数即可</description>
		  </property>  
		  <property>
		    <name>dfs.webhdfs.enabled</name>
		    <value>true</value>
		  </property>
		  <property>      
		    <name>dfs.nameservices</name>    
		    <value>cats</value> 
		    <description>集群的逻辑名称，自己任意取</description>     
		  </property>  
		  <property>  
		    <name>dfs.ha.namenodes.cats</name>  
		    <value>catk,catq</value>
		    <description>两个namenode的逻辑名称，自己任意取</description> 
		  </property>  
		  <property>  
		    <name>dfs.namenode.rpc-address.cats.catk</name>  
		    <value>10.2.2.141:8020</value>  
		  </property>  
		  <property>      
		    <name>dfs.namenode.http-address.cats.catk</name>      
		    <value>10.2.2.141:50070</value>      
		  </property>      
		  <property>      
		    <name>dfs.namenode.rpc-address.cats.catq</name>      
		    <value>10.2.2.142:8020</value>      
		  </property>  
		  <property>      
		    <name>dfs.namenode.http-address.cats.catq</name>      
		    <value>10.2.2.142:50070</value>     
		  </property>
		  <property>
		    <name>dfs.namenode.servicerpc-address.cats.catk</name>  
		    <value>10.2.2.141:53310</value>  
		  </property>    
		  <property>  
		    <name>dfs.namenode.servicerpc-address.cats.catq</name>  
		    <value>10.2.2.142:53310</value>  
		  </property>
		  <property>    
		    <name>dfs.ha.automatic-failover.enabled</name>    
		    <value>true</value> 
		    <description>指定cats是否自动故障恢复，当NameNode出故障时，是否自动切换到另一台NameNode</description>
		  </property>     
		  <!--指定JournalNode -->  
		  <property>  
		    <name>dfs.namenode.shared.edits.dir</name>       
		    <value>qjournal://10.2.2.144:8485;10.2.2.145:8485;10.2.2.146:8485;10.2.2.147:8485;10.2.2.148:8485;10.2.2.149:8485;10.2.2.150:8485;10.2.2.151:8485;10.2.2.152:8485/cats</value> 
		    <description>两个NameNode为了数据同步，会通过一组称作JournalNodes的独立进程进行相互通信。当active状态的NameNode的命名空间有任何修改时，会告知大部分的JournalNodes进程。standby状态的NameNode有能力读取JNs中的变更信息，并且一直监控editl og的变化，把变化应用于自己的命名空间。standby可以确保在集群出错时，命名空间状态已经完全同步了。指定cats的两个NameNode共享edits文件目录时，使用的JournalNode集群信息，必须是奇数个，至少3个。</description>  
		  </property>  
		  <property>  
		    <name>dfs.client.failover.proxy.provider.cats</name>       
		    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
		    <description>指定active出故障时，哪个实现类负责执行故障切换</description>  
		  </property>  
		  <property>      
		    <name>dfs.journalnode.edits.dir</name>      
		    <value>/opt/hadoop-2.7.3/var/journal</value>      
		  </property>
		  <property>      
		    <name>dfs.ha.fencing.methods</name>      
		    <value>sshfence</value>  
		    <description>主备架构解决单点故障问题时，必须要认真解决的是脑裂问题，即出现两个master同时对外提供服务，导致系统处于不一致状态，可能导致数据丢失等潜在问题。在HDFS HA中，JournalNode只允许一个NameNode写数据，不会出现两个Active NameNode的问题，但是，当主备切换时，之前的Active NameNode可能仍在处理客户端的RPC请求，为此，需要增加隔离机制（ fencing）将之前的Active NameNode杀死。HDFS允许用户配置多个隔离机制，当发生主备切换时，将顺次执行这些隔离机制，直到一个返回成功。隔离机制包括shell和sshfence，sshfence通过ssh登录到前一个ActiveNameNode并将其杀死。为了让该机制成功执行，需配置免密码ssh登陆</description>
		  </property>  
		  <property>      
		    <name>dfs.ha.fencing.ssh.private-key-files</name>      
		    <value>/home/cat/.ssh/id_rsa</value>   
		  </property>  
		  <property>  
		    <name>dfs.ha.fencing.ssh.connect-timeout</name>  
		    <value>5000</value>  
		    <description>设置一个超时时间，一旦ssh超过该时间，则认为执行失败</description>
		  </property>
		  <property>  
		    <name>dfs.namenode.handler.count</name>  
		    <value>20</value> 
		    <description>设定namenode server threads的数量，这些threads會用RPC跟其他的datanodes沟通。当datanodes数量太多时会发現很容易出現RPC timeout，解決方法是提升网络速度或提高这个值，但要注意的是thread数量多也表示namenode消耗的内存也随着增加</description> 
		  </property> 
		  <!-- 用户权限管理 -->
		  <property>  
		    <name>dfs.permissions.enabled</name>  
		    <value>true</value>
		    <description>权限管理</description>
		  </property>  
		  <property>
		    <name>dfs.namenode.acls.enabled</name>
		    <value>true</value>
		    <description>权限访问控制列表</description>
		  </property>
		  <!-- 用户权限管理 -->
		</configuration>

4. etc/hadoop/mapred-env.sh

		export JAVA_HOME=/opt/jdk1.8.0_121
		export HADOOP_MAPRED_PID_DIR=/opt/hadoop-2.7.3/var/tmp

5. etc/hadoop/mapred-site.xml

		<configuration>
		  <property>  
		    <name>mapreduce.framework.name</name>  
		    <value>yarn</value>  
		  </property>
		  <property>  
		    <name>mapreduce.map.memory.mb</name>  
		    <value>1536</value>  
		    <description>每个Map Task需要的虚拟内存限制，例如需要1536MB内存时，yarn将分配一个2倍最小容器内存2048M单位的容器，这个值可以在程序中设定被覆盖</description>
		  </property>
		  <property>  
		    <name>mapreduce.reduce.memory.mb</name>  
		    <value>1536</value>  
		    <description>每个Reduce Task需要的虚拟内存限制，一般为Map的两倍，这个值可以在程序中设定被覆盖</description>
		  </property>
		  <property>  
		    <name>mapreduce.map.java.opts</name>  
		    <value>-Xmx1024m -XX:MaxPermSize=64m</value>  
		    <description>设置Map Task的JVM的堆空间大小，因为Map Task分配了4096MB内存，其中3596MB内存给了java、scala等程序，这个值应该比上面的task的虚拟内存值小（因为jvm除了heap还有别的对象需要占用内存），如果jvm进程在执行中heap上的对象占用内存超过这个值， 则会抛出OutOfMemory Exception，这个值可以在程序中设定被覆盖</description>
		  </property>
		  <property>  
		    <name>mapreduce.reduce.java.opts</name>  
		    <value>-Xmx1024m -XX:MaxPermSize=64m</value>  
		    <description>设置Reduce Task的JVM的堆空间大小，这个值可以在程序中设定被覆盖</description>
		  </property>
		  <property>  
		    <name>mapreduce.task.io.sort.mb</name>  
		    <value>512</value>  
		    <description>任务内部排序缓冲区大小</description>
		  </property>
		</configuration>

6. etc/hadoop/slaves

		# datanode的ip地址
		10.2.2.143...152

7. etc/hadoop/yarn-env.sh

		export JAVA_HOME=/opt/jdk1.8.0_121
		# 因为使用了spark_shuffle，因此需要提高YARN_HEAPSIZE，避免在shuffle过程中的garbage collection问题
		YARN_HEAPSIZE=5000

8. etc/hadoop/yarn-site.xml

		<configuration>
		  <!-- ResourceManager相关配置参数 -->
		  <property>
		  	<name>yarn.scheduler.minimum-allocation-mb</name>
		  	<value>1024</value>
		  	<description>每个Task申请容器的最小内存单位，默认1024MB，例如Task需要1025M内存时会分配2*1024M内存的容器</description>
		  </property>
		  <property>
			<name>yarn.scheduler.maximum-allocation-mb</name>
			<value>12288</value>
			<discription>单个任务可申请容器的最大内存，默认8192MB。由于Yarn集群还需要跑Spark的任务，而Spark的Worker内存相对需要大些，所以需要调大单个任务的最大内存</discription>
		  </property>
		  <property>
			<name>yarn.scheduler.maximum-allocation-vcores</name>
			<value>8</value>
			<discription>单个任务可申请容器的最大虚拟核数，默认8个核</discription>
		  </property>
		  <property>      
		    <name>yarn.resourcemanager.hostname</name>      
		    <value>10.2.2.141</value>  
		    <description>resourcemanager只有一个，设置它的主机位置</description>
		  </property>  
		  <property>
		    <name>yarn.resourcemanager.address</name>
		    <value>10.2.2.141:8032</value>
		    <description>ResourceManager对客户端暴露的地址。客户端通过该地址向RM提交应用程序，杀死应用程序等</description>
		  </property>
		  <property>
		    <name>yarn.resourcemanager.scheduler.address</name>
		    <value>10.2.2.141:8030</value>
		    <description>ResourceManager对ApplicationMaster暴露的访问地址。ApplicationMaster通过该地址向RM申请资源、释放资源等</description>
		  </property>
		  <property>
		    <name>yarn.resourcemanager.resource-tracker.address</name>
		    <value>10.2.2.141:8031</value>
		    <description>ResourceManager对NodeManager暴露的地址。NodeManager通过该地址向ResourceManager汇报心跳，领取任务等</description>
		  </property>
		  <property>
		    <name>yarn.resourcemanager.admin.address</name>
		    <value>10.2.2.141:8033</value>
		    <description>ResourceManager对管理员暴露的访问地址。管理员通过该地址向RM发送管理命令等</description>
		  </property>
		  <property>
		    <name>yarn.resourcemanager.webapp.address</name>
		    <value>10.2.2.141:8088</value>
		    <description>ResourceManager对外web ui地址。用户可通过该地址在浏览器中查看集群各类信息</description>
		  </property>
		  <!-- NodeManager相关配置参数 -->
		  <property>
		  	<name>yarn.nodemanager.resource.memory-mb</name>
		  	<value>12288</value>
		  	<description>NodeManager总的可用物理内存，必须给物理机留一些内存和计算资源给到操作系统使用，不能用光，16G*85%</description>
		  </property>
		  <property>
		  	<name>yarn.nodemanager.resource.cpu-vcores</name>
		  	<value>8</value>
		  	<description>该节点上YARN可使用的虚拟CPU个数，默认是8，注意，目前推荐将该值设值为与物理CPU核数数目相同。如果你的节点CPU核数不够8个，则需要调减小这个值，而YARN不会智能的探测节点的物理CPU总数</description>
		  </property>
		  <property>
		  	<name>yarn.nodemanager.vmem-pmem-ratio</name>
		  	<value>2.1</value>
		  	<description>当map或reduce任务使用内存超过mapreduce.map.memory.mb或mapreduce.reduce.memory.mb时，将增加内存至2.1倍的mapreduce.reduce.memory.mb或者mapreduce.map.memory.mb</description>
		  </property>
		  <property>  
		    <name>yarn.nodemanager.aux-services</name>  
		    <value>mapreduce_shuffle,spark_shuffle</value>  
		  </property>
		  <property>
		    <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
            <value>org.apache.hadoop.mapred.ShuffleHandler</value>
		  </property>
		  <property>
		    <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
		    <value>org.apache.spark.network.yarn.YarnShuffleService</value>
		  </property>
		  <property>  
		    <name>yarn.resourcemanager.scheduler.class</name>
		    <value>	org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
		    <description>启用的资源调度器主类。目前可用的有FIFO、Capacity Scheduler和Fair Scheduler</description>
		  </property>
		  <property>
		    <name>yarn.nodemanager.remote-app-log-dir</name>
		    <value>/tmp/logs</value>
		  </property>
		  <!-- ApplicationMaster相关配置参数 -->
		  <property>  
		    <name>yarn.app.mapreduce.am.resource.mb</name>
		    <value>1536</value>
		    <description>MR ApplicationMaster占用的内存量</description>
		  </property>
		  <property>  
		    <name>yarn.app.mapreduce.am.command-opts</name>
		    <value>-Xmx1024m -XX:MaxPermSize=64m</value>
		  </property>
		  <!-- historyserver configue start -->
		  <property>
		    <name>yarn.log-aggregation-enable</name>
		    <value>true</value>
		    <description></description>
		  </property>
		  <!-- historyserver configue end -->
		</configuration>

9. 分发Hadoop配置文件

		$ ansible slave -m copy -a 'src=/opt/hadoop-2.7.3/etc/hadoop/ dest=/opt/hadoop-2.7.3/etc/hadoop/'

---

### 启动Hadoop
---

1. 确保Zookeeper已经启动：

		$ ansible zookeeper -a '/opt/zookeeper-3.4.9/bin/zkServer.sh start'

2. (在主节点)格式化集群

		$ hdfs zkfc -formatZK

3. (在主节点)检验集群是否格式化，ls /是否出现了hadoop-ha

		$ zkCli.sh
		[zk: localhost:2181(CONNECTED) 0] ls /
		[zookeeper, hadoop-ha]

4. (在主节点)启动hadoop集群 

	1. 启动所有journalnode

			$ hadoop-daemons.sh start journalnode

	2. 在cat1格式化namenode，输出日志中是否出现successfully formatted

			cat@cat1:~$ hdfs namenode -format

	3. 启动cat1中的namenode

			cat@cat1:~$ hadoop-daemon.sh start namenode

	4. 在cat2中同步namenode数据

			cat@cat2:~$ hdfs namenode -bootstrapStandby

	5. 启动cat2中的namenode

			cat@cat2:~$ hadoop-daemon.sh start namenode

	6. 启动所有datanode

			$ hadoop-daemons.sh start datanode

	7. 启动yarn

			$ start-yarn.sh

	8. 在cat1和cat2中都启动ZooKeeperFailoverController（此时cat1变为active，cat2为standby）

			$ ansible namenode -a '/opt/hadoop-2.7.3/sbin/hadoop-daemon.sh start zkfc'

---

### 测试Hadoop
---

1. 使用jps查看各节点起的进程

		$ ansible all -a '/opt/jdk1.8.0_121/bin/jps'

		cat1: QuorumPeerMain, DFSZKFailoverController, NameNode, ResourceManager
		cat2: QuorumPeerMain, DFSZKFailoverController, NameNode
		cat3: QuorumPeerMain, JournalNode, DataNode, NodeManager
		cat4-12: JournalNode, DataNode, NodeManager

2. 查看[http://10.2.2.141:50070/](http://10.2.2.141:50070/)中cat1是否为active，查看[http://10.2.2.142:50070/](http://10.2.2.142:50070/)中cat2是否为standby

3. (在主节点)运行wordcount例子:
		
		$ hadoop fs -mkdir /test
		$ hadoop fs -mkdir /test/input
		$ hadoop fs -put ./words.txt /test/input
		$ hadoop jar /opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount /test/input/ /test/output
		$ hadoop fs -cat /test/output/part-r-00000

---

### 创建Hadoop用户
---

1. 创建用户组

		$ sudo groupadd admire

2. 创建用户

		$ sudo useradd -g admire -d /home/wyn -m wyn -s /bin/bash
		$ sudo passwd wyn
		$ hadoop fs -mkdir /user/wyn
		$ hadoop fs -chown -R wyn:supergroup /user/wyn
		$ hadoop fs -chmod 700 /user/wyn

---

## 下一步：[安装Spark on Yarn](https://wuyinan0126.github.io/2017/大数据学习笔记(6)-Spark/)

---