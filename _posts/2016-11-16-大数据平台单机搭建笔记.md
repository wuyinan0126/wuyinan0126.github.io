---
title:  "<font color='red'>[原创]</font> 大数据平台单机搭建笔记"
date:   2016-11-16 00:00:00
categories: [原创]
tags: [原创]
---

**

## /etc/profile
---

	export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home
	export HADOOP_HOME=/opt/hadoop-2.7.3
	export SCALA_HOME=/opt/scala-2.11.8
	export SPARK_HOME=/opt/spark-2.1.0
	# Spark运行必须export下面三个变量
	export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
	export HDFS_CONF_DIR=$HADOOP_HOME/etc/hadoop
	export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop

	export HIVE_HOME=/opt/hive-2.1.0

	export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SCALA_HOME/bin:$SPARK_HOME/bin:$SPARK_HOME/sbin:$HIVE_HOME/bin:/usr/local/sbin:$PATH

## 安装Hadoop（版本2.7.3）
---

#### 准备工作 
	1. SSH免密码登录:
		1. 生成密钥: 
			ssh-keygen -t dsa
		2. 生成authorized_keys，进入.ssh目录:
			cat id_dsa.pub >> authorized_keys
			chmod 600 authorized_keys
		3. 测试:
			ssh localhost
	2. 下载Hadoop二进制包（或源码自行编译），解压至/opt/:
		tar -zxvf ./hadoop-2.7.3.tar.gz -C /opt/

#### 配置Hadoop
	1. etc/hadoop/core-site.xml:
		<configuration>
			<property>
				<name>fs.defaultFS</name>
				<value>hdfs://localhost:9000</value>
				<description>默认的HDFS路径，与hdfs-site.xml中的配置相关。当有多个HDFS集群存在时，用此名字指定集群</description>
			</property>
			<property>
				<name>hadoop.tmp.dir</name>
				<value>/opt/hadoop-2.7.3/var/tmp</value> 
				<description>NameNode、DataNode、JournalNode等存放数据的公共目录，也可以自己单独指定这三类节点的目录</description>
			</property>
			<property>
				<name>hadoop.proxyuser.wuyinan.groups</name>
				<value>*</value>
				<description></description>
			</property>
			<property>
				<name>hadoop.proxyuser.wuyinan.hosts</name>
				<value>*</value>
				<description></description>
			</property>
		</configuration>

	2. etc/hadoop/hdfs-site.xml:
		<configuration>
			<property>
				<name>dfs.namenode.name.dir</name>
				<value>/opt/hadoop-2.7.3/var/name</value>
				<description>NameNode存储命名空间和操作日志相关的元数据信息的本地文件系统路径</description>
			</property>
			<property>
				<name>dfs.datanode.data.dir</name>
				<value>/opt/hadoop-2.7.3/var/data</value>
				<description>DataNode节点存储HDFS文件的本地文件系统路径</description> 
			</property>
			<property>
				<name>dfs.replication</name>
				<value>1</value>
				<description>指定DataNode存储block的副本数量。默认值是3个，不大于DataNode个数即可</description>
			</property>
		</configuration>

	3. etc/hadoop/mapred-site.xml:
		<configuration>
			<property>
				<name>mapreduce.framework.name</name>
				<value>yarn</value> 
				<description>指定运行mapreduce的环境是yarn</description>
			</property>
			<property>
				<name>mapreduce.tasktracker.map.tasks.maximum</name>
				<value>2</value>
				<description></description>
			</property>
			<property>
				<name>mapreduce.tasktracker.reduce.tasks.maximum</name>
				<value>2</value>
				<description></description>
			</property>
		</configuration>

	4. etc/hadoop/yarn-site.xml:
		<configuration>
			<property>      
				<name>yarn.resourcemanager.hostname</name>      
				<value>localhost</value>  
				<description>指定ResourceManager的地址</description>
			</property>  
			<property>  
				<name>yarn.nodemanager.aux-services</name>  
				<value>mapreduce_shuffle</value>  
				<description></description>
			</property>
			<property>  
				<name>yarn.resourcemanager.scheduler.class</name>
				<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
				<description></description>
			</property>
			<property>
				<name>yarn.log-aggregation-enable</name>
				<value>true</value>
				<description></description>
			</property>
			<property>
				<name>yarn.nodemanager.remote-app-log-dir</name>
				<value>/tmp/logs</value>
				<description></description>
			</property>
		</configuration>

	5. etc/hadoop/mapred-env.sh:
		export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home
		export HADOOP_MAPRED_PID_DIR=/opt/hadoop-2.7.3/var/tmp

	6. etc/hadoop/hadoop-env.sh:
		export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home
		export HADOOP_PID_DIR=/opt/hadoop-2.7.3/var/tmp
		export HADOOP_SECURE_DN_PID_DIR=/opt/hadoop-2.7.3/var/tmp

	7. etc/hadoop/slaves:
		localhost

#### 启动Hadoop	
	1. 根据hadoop.tmp.dir, dfs.namenode.name.dir, dfs.datanode.name.dir在相应位置创建目录
		mkdir /opt/hadoop-2.7.3/var/
		mkdir /opt/hadoop-2.7.3/var/tmp
		mkdir /opt/hadoop-2.7.3/var/name
		mkdir /opt/hadoop-2.7.3/var/data
	2. 格式化namenode，如果Exiting with status 0，说明格式化成功:
		hdfs namenode -format
	3. 启动namenode、datanode、yarn，每启动一个后都用jps查看是否启动成功，最后启动的进程应该包括NameNode、DataNode、ResorceManager、NodeManager:
		hadoop-daemon.sh start namenode
		hadoop-daemon.sh start datanode
		start-yarn.sh
	4. 在浏览器查看：
		http://localhost:50070/
		http://localhost:8088/
	5. 测试，运行wordcount例子:
		hadoop fs -mkdir /test
		hadoop fs -mkdir /test/input
		hadoop fs -put ./words.txt /test/input
		hadoop jar /opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount /test/input/ /test/output
		hadoop fs -cat /test/output/part-r-00000

## 安装Scala（版本2.11.8）
---
	1. 下载Scala二进制包，解压至/opt/:
		tar -zxvf ./scala-2.11.8.tgz -C /opt/

## 安装Spark（版本2.1.0）
---

#### 准备工作 
1. 下载Spark二进制包（或源码自行编译），解压至/opt/:

		tar -zxvf ./spark-2.1.0-bin-hadoop2.7 -C /opt/
		mv /opt/spark-2.1.0-bin-hadoop2.7 /opt/spark-2.1.0

#### 配置Spark
1. conf/spark-env.sh:

		export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home
		export HADOOP_HOME=/opt/hadoop-2.7.3
		export SCALA_HOME=/opt/scala-2.11.8	

2. conf/slaves:

		localhost

#### 启动Spark	
1. 启动Spark，用jps查看，启动的进程应包括Master、Worker:

		start-all.sh

2. 测试，运行SparkPi例子:
		cd /opt/spark-2.1.0
		./bin/spark-submit --class org.apache.spark.examples.SparkPi \
		--master yarn-cluster \
		--num-executors 3 \
		--driver-memory 2g \
		--executor-memory 1g \
		--executor-cores 1 \
		--queue thequeue \
		examples/jars/spark-examples_2.11-2.1.0.jar \
		10
3. 查看结果:

		yarn logs -applicationId ${applicationId} | grep 'Pi'


