---
title:  "<font color='red'>[原创]</font> 大数据学习笔记(7)-数据仓库"
date:   2017-01-26 00:00:00
categories: [原创,大数据]
tags: [原创,大数据]
---

**

## 安装Hive（版本2.1.0）on Tez（版本0.8.5）
---

### 准备工作 
---

1. 按照[大数据学习笔记(2)-Hadoop](https://wuyinan0126.github.io/2016/大数据学习笔记(2)-Hadoop/)做好部署前提准备

2. 下载Hive二进制包，解压至/opt/:

		$ ansible all -m copy -a 'src=/home/cat/cat/apache-hive-2.1.0-bin.tar.gz dest=/home/cat/cat/'
		$ ansible all -a 'tar -zxvf /home/cat/cat/apache-hive-2.1.0-bin.tar.gz -C /opt/'
		$ ansible all -a 'mv /opt/apache-hive-2.1.0-bin /opt/hive-2.1.0'

3. 下载Mysql驱动包[https://dev.mysql.com/downloads/connector/j/](https://dev.mysql.com/downloads/connector/j/)，并放入/opt/hive-2.1.0/lib中

		$ ansible slave -m copy -a 'src=/opt/hive-2.1.0/lib/ dest=/opt/hive-2.1.0/lib/'

4. (在主节点)安装MySQL用于存储元数据

		$ sudo apt-get install mysql-server

5. 为Hive建立相应的MySQL账户,并赋予足够的权限:

		$ mysql -uroot -p
		mysql> CREATE USER 'hive' IDENTIFIED BY 'yourpassword';
		mysql> GRANT ALL PRIVILEGES ON *.* TO 'hive'@'%' WITH GRANT OPTION;
		mysql> flush privileges;

6. [可选] 解除Mysql只允许本地登录，/etc/mysql/mysql.conf.d/mysqld.cnf :

		#bind-address           = 127.0.0.1

7. 重新加载配置文件

		$ sudo service mysql restart
		#在MacOS中
		$ launchctl unload /Users/wuyinan/Library/LaunchAgents/homebrew.mxcl.mysql.plist
		$ launchctl load /Users/wuyinan/Library/LaunchAgents/homebrew.mxcl.mysql.plist

8. 建立Hive专用的元数据库:

		$ mysql -uhive -p
		mysql> create database metastore;
		mysql> USE metastore;
		mysql> SOURCE /opt/hive-2.1.0/scripts/metastore/upgrade/mysql/hive-schema-2.1.0.mysql.sql;

9. 在HDFS创建数据存储仓库

		$ hadoop fs -mkdir -p /user/hive/warehouse
		$ hadoop fs -chmod -R 777 /user/hive

10. 创建hive的log文件存储目录

		$ mkdir /opt/hive-2.1.0/logs

---

### 配置Hive
---

1. 增加conf/hive-site.xml文件::

		<?xml version="1.0" encoding="UTF-8" standalone="no"?>
		<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
		<configuration>
		  <property>
		    <name>javax.jdo.option.ConnectionURL</name>
		    <value>jdbc:mysql://10.2.2.141:3306/metastore?characterEncoding=UTF-8&amp;useSSL=false</value>
		    <description>元数据存储的Mysql路径</description>
		  </property>
		  <property>
		    <name>javax.jdo.option.ConnectionDriverName</name>
		    <value>com.mysql.jdbc.Driver</value>
		  </property>
		  <property>
		    <name>javax.jdo.option.ConnectionUserName</name>
		    <value>hive</value>
		  </property>
		  <property>
		    <name>javax.jdo.option.ConnectionPassword</name>
		    <value>yourpassword</value>
		  </property>
		  <property>
		     <name>hive.metastore.warehouse.dir</name>
		     <value>/hive/warehouse</value>
		     <description>数据存储的HDFS路径</description>
		  </property>
		  <property>
		    <name>hive.metastore.uris</name>
		    <value>thrift://10.2.2.141:9083</value>
		    <description>spark-shell默认将启动一个SqlContext，因此在{SPARK_HOME}/conf目录下需要有该hive-site.xml文件，并且文件中需要指定hive.metastore.uris</description>
		  </property>
		</configuration>

2. 增加conf/hive-env.sh文件:
		
		export HADOOP_HOME=/opt/hadoop-2.7.3
		export HIVE_CONF_DIR=/opt/hive-2.1.0/conf	
		# tez相关
		export TEZ_HOME=/opt/tez-0.8.5
		export TEZ_CONF_DIR=/opt/tez-0.8.5/conf
		export TEZ_JARS=/opt/tez-0.8.5
		export HADOOP_CLASSPATH=${TEZ_CONF_DIR}:${TEZ_JARS}/*:${TEZ_JARS}/lib/*

3. 增加conf/hive-log4j2.properties文件

		property.hive.log.dir = /opt/hive-2.1.0/logs

4. 将/opt/hive-2.1.0/conf/hive-site.xml文件和/opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml文件复制到/opt/spark-2.1.0/conf下

		$ cp /opt/hive-2.1.0/conf/hive-site.xml /opt/spark-2.1.0/conf/
		$ cp /opt/hadoop-2.7.3/etc/hadoop/hdfs-site.xml /opt/spark-2.1.0/conf/

5. 分发Hive配置文件

		$ ansible slave -m copy -a 'src=/opt/hive-2.1.0/conf/ dest=/opt/hive-2.1.0/conf/'
		$ ansible slave -m copy -a 'src=/opt/spark-2.1.0/conf/ dest=/opt/spark-2.1.0/conf/'

---

### 启动Hive
---

1. 启动Hadoop

2. (在主节点)启动Hive服务器

		$ hive --service metastore &
	
---

### 测试Hive
---

1. (在任一节点)启动Hive，进入Hive交互界面:

		$ hive
		hive>
		hive> show databases;
		hive> use default;
		hive> create table test(id INT);
		hive> insert into test VALUES(0);
		hive> select count(*) from test;

---

## 安装Tez（版本0.8.5）
---

### 准备工作 
---

1. 下载Tez源码包，修改pom.xml：

		<hadoop.version>2.7.3</hadoop.version>
		<javac.version>1.8</javac.version>

2. 根据pom.xml中protobuf.version，安装protobuf：

		$ wget https://github.com/google/protobuf/releases/download/v2.5.0/protobuf-2.5.0.tar.gz
		$ tar -zxvf protobuf-2.5.0.tar.gz -C .
		$ cd protobuf-2.5.0/
		$ ./configure
		$ make
		$ make check
		$ sudo make install
		$ sudo vi /etc/ld.so.conf.d/bprotobuf.conf
		/usr/local/lib
		$ sudo ldconfig
		$ protoc --version
		libprotoc 2.5.0

3. 修改Maven镜像源/etc/maven/settings.xml，在mirros下添加子节点

	    <mirror>
	        <id>nexus-aliyun</id>
	        <mirrorOf>*</mirrorOf>
	        <name>Nexus aliyun</name>
	        <url>http://maven.aliyun.com/nexus/content/groups/public</url>
	    </mirror> 

4. 开代理，编译：

		$ export http_proxy=http://127.0.0.1:8123/
		$ export https_proxy=http://127.0.0.1:8123/
		$ mvn package -DskipTests=true -Dmaven.javadoc.skip=true

5. 服务端使用完整包。将编译好的tez-dist/target/tez-0.8.5.tar.gz上传到hdfs上(/user/tez/)

		$ hadoop fs -mkdir /user/tez/
		$ hadoop fs -put ./tez-dist/target/tez-0.8.5.tar.gz
		$ hadoop fs -chmod -R 777 /user/tez

6. 客户端使用minimal包。将编译好的tez-dist/target/tez-0.8.5-minimal.tar.gz分发到各节点，解压

		$ ansible all -m copy -a "src=./tez-dist/target/tez-0.8.5-minimal.tar.gz dest=/home/cat/cat/"
		$ ansible all -a "mkdir /opt/tez-0.8.5"
		$ ansible all -a "tar -zxvf /home/cat/cat/tez-0.8.5-minimal.tar.gz -C /opt/tez-0.8.5/"

---

### 配置Tez
---

1. conf/tez-site.xml
		
		<configuration>
		  <property>
		    <name>tez.lib.uris</name>
		    <value>${fs.defaultFS}/user/tez/tez-0.8.5-minimal.tar.gz</value>
		  </property>
		  <property>
		    <name>tez.use.cluster.hadoop-libs</name>
		    <value>true</value>
		  </property>
		</configuration>

2. 分发配置

		ansible all -m copy -a "src=/opt/tez-0.8.5/conf/ dest=/opt/tez-0.8.5/conf/"

---

### 测试Tez
---

1. (在任一节点)启动Hive，进入Hive交互界面:

		$ hive
		hive>
		hive> set hive.execution.engine=tez;
		hive> use default;
		hive> select count(*) from test;

---

## 安装Sqoop（版本1.4.6）
---

### 准备工作
---

1. 下载Sqoop二进制包，解压至/opt/:

		$ ansible all -m copy -a 'src=/home/cat/cat/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz dest=/home/cat/cat/'
		$ ansible all -a 'tar -zxvf /home/cat/cat/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /opt/'
		$ ansible all -a 'mv /opt/sqoop-1.4.6.bin__hadoop-2.0.4-alpha /opt/sqoop-1.4.6'

2. 将pg-jdbc和mysql的jar包放入$SQOOP_HOME/lib

		[https://jdbc.postgresql.org/download.html](https://jdbc.postgresql.org/download.html)		
		[https://dev.mysql.com/downloads/connector/j/](https://dev.mysql.com/downloads/connector/j/)

		$ ansible slave -m copy -a 'src=/opt/sqoop-1.4.6/lib/ dest=/opt/sqoop-1.4.6/lib/'
---

### 配置Sqoop
---

1. conf/sqoop-env.sh

		export HADOOP_COMMON_HOME=/opt/hadoop-2.7.3
		export HIVE_HOME=/opt/hive-2.1.0

2. 分发Sqoop配置文件

		$ ansible slave -m copy -a 'src=/opt/sqoop-1.4.6/conf/ dest=/opt/sqoop-1.4.6/conf/'

---

### 测试Sqoop
---

1. (在任一节点):
	
		$ sqoop list-databases --connect jdbc:postgresql://10.2.26.96:5432/lrs --username lrs_owner --password pass
		$ sqoop list-databases --connect jdbc:mysql://10.2.2.141:3306/metastore --username hive --password pass

2. psql向hdfs的导入，HDFS中的路径文件夹必需不存在:

		$ sqoop import --connect jdbc:postgresql://{psql的IP地址}:5432/{数据库名} --username {用户名} --password {密码} --table {表名} --target-dir {HDFS中的路径} -m 5
		
		$ sqoop import --connect jdbc:postgresql://10.2.3.100:5432/cat --username postgres --password ada --table kitty --target-dir /test/kitty -m 5

3. psql向hive的导入:

		$ sqoop import --connect jdbc:postgresql://{psql的IP地址}:5432/{数据库名} --username {用户名} --password {密码} --table {表名} --hive-import -m 5
		
		$ sqoop import --connect jdbc:postgresql://10.2.3.100:5432/cat --username postgres --password ada --table kitty --hive-import -m 5

---
