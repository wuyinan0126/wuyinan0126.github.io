<p><em>软件定义广域网B4</em></p>

<h2 id="section">文章</h2>

<blockquote>
  <p><a href="[http://course.buaa.edu.cn/access/content/attachment/68d3021a-8007-406f-be28-6ff8f2c8e8f2/%E4%BD%9C%E4%B8%9A/ac8eb7d9-03c5-48df-bc92-310e400d6be5/B4%20p3-jain.pdf]">B4: Experience with a Globally-Deployed Software Defined WAN</a></p>
</blockquote>

<h2 id="section-1">引言：从流量浪费说起</h2>
<hr />

<p>数据中心的网络流量平时只能达到峰值的30%～40%水平，但是为了保证峰值时期的带宽需求，数据中心往往购买大量带宽和高端路由设备以保证数据中心的健壮性。但这大大提高了数据中心成本，因此Google公司根据数据中心流量大优先级使用一个中心控制系统来分配带宽。</p>

<p>数据中心流量特点：</p>

<ol>
  <li>用户数据流量。数据量小，延迟敏感高，因此优先级最高；</li>
  <li>存储访问流量。数据量较大，延迟敏感较不敏感；</li>
  <li>大规模同步数据流量，用以保证多个数据中心的一致性。数据流量最大，延迟敏感最低，因此优先级最低。</li>
</ol>

<p>因此只要保证高优先级流量低延迟到达，同时低优先级流量充满流量低谷，那么数据中心的广域网连接就能达到100%的利用率。</p>

<h2 id="sdn">解决方案：软件定义网络（SDN）</h2>
<hr />

<p>网络路由控制从一开始的手动配置路由规则，到分布式、自组织的路由协议BGP、OSPF的出现，保证了网络的健壮性。但是随着数据中心规模的不断扩大，交换机、服务器数量的不断增长，网络设备和网络流量的管理越来越复杂，因此SDN应运而生。</p>

<p>以OpenFlow为代表，SDN把分散的自主路由控制集合起来，路由器按照控制器指定的规则对分组进行处理，并执行相应操作。这样，控制器就可以利用整个网络的拓扑信息和来自应用的需求信息计算出一组接近全局最优的路由规则。这种集中的Traffic Engineering（TE）有几个好处：</p>

<ol>
  <li>使用非最短路径的分组转发策略，将应用的优先级纳入资源分配的考虑；</li>
  <li>当链路或交换机出现故障，或者应用的需求发生变化时，动态地重新分配带宽；</li>
  <li>在较高层次（如应用层）上制定规则。</li>
</ol>

<h2 id="b4">设计：B4架构</h2>
<hr />

<p><img src="/assets/2016-10-29-1.png" alt="B4架构图" title="B4架构图" /></p>

<p>B4的整体架构如上图，</p>

<ul>
  <li>global层由逻辑上集中的应用（例如，SDN网关和中央TE（Traffic Engineering）服务器）构成，它使得能通过站点级的网络控制应用（Network Control Application，NCA）对整个网络进行中央控制。</li>
  <li>site controllers层为每个数据中心（Site）的控制器，其上运行着OpenFlow Controller（OFC）集群，根据网络控制应用的指令和交换机事件，维护网络状态。</li>
  <li>switch hardware层是交换机硬件，只负责转发流量，不运行复杂的控制软件，其上运行着OpenFlow Agent（OFA），接受OFC的指令并将TE规则写到硬件flow-table里。</li>
</ul>

<h3 id="section-2">交换机的设计</h3>

<p><img src="/assets/2016-10-29-2.png" alt="交换机和结构图" title="交换机和结构图" /></p>

<p>Google定制的128口交换机由24个16口10Gbps通用交换机芯片制成。蓝色的芯片是对外插线的，绿色的芯片充当背板（backplane）。如果发往蓝色芯片的分组目标MAC地址在同一块蓝色芯片上，就“内部解决”；如果不是，则发往背板，由绿色芯片发到目标地址所对应的蓝色芯片。</p>

<p>交换机上运行着OpenFlow客户端进程OFA（OpenFlow Agent)，连接到远程的OFC（OpenFlow Controller），接受OpenFlow命令，转发特定的分组和连接状态到OFC。例如，BGP协议的分组会被转发到OFC上，在OFC上运行BGP协议栈。</p>

<h3 id="section-3">路由</h3>

<p><img src="/assets/2016-10-29-3.png" alt="路由图" title="路由图" /></p>

<p>如上图，Quagga层中的RIB（Routing Information Base）保存着路由所需要的网络拓扑、路由规则，决定分组的转发路径。在OFC层中，RIB被分为Flows和ECMP Groups。Flows用于地址匹配，而ECMP则是对匹配的分组进行操作（如修改分组头部、减少TTL、发送或丢弃等），并使用Equal Cost Multiple Path(ECMP)协议，从多条最短路中随机选一条走。</p>

<h3 id="te">TE优化算法</h3>

<p>使用四元组{Source site, Dest site, Priority, Required bandwidth}定义每个应用在每对数据中心之间所需的带宽和优先级，并将这些四元组按照{Source site, Dest site, Priority}分组，把所需带宽加起来，就形成了一系列Flow Group（FG）。每个FG由四元组{Source site, Dest site, Priority, Bandwidth}表征。</p>

<p>TE优化算法的目标是最大公平（max-min fairness），就是在公平的前提下满足尽可能多的需求。我们假定，客户的满意度是跟提供的带宽成正比的，也是跟优先级成反比的（优先级越高，就越不容易被满足，满意度越低）；如果已经提供了所有要求的带宽，则满意度不再提高。在此假设基础上，引入fair share（公平共享）的概念，两个Flow Group如果fair share相同，就认为这两个客户的满意度相同，是公平的。</p>

<p>TE优化算法分下面三步：</p>

<ol>
  <li>Tunnel Selection: 选择每个FG可能走的几条路线（tunnel）</li>
  <li>Tunnel Group Generation: 根据带宽函数给FG分配带宽</li>
  <li>Tunnel Group Quantization: 将带宽离散化到硬件可以实现的粒度</li>
</ol>

<p>举例：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>App1需要15G带宽，优先级为10，从Site A到Site B，因此可以表示为{A, B, 10, 15}；
App2需要5G带宽，优先级为1，从Site A到Site B，因此可以表示为{A, B, 1, 5}；
App3需要5G带宽，优先级为0.5，从Site A到Site C，因此可以表示为{A, B, 0.5, 5}；
</code></pre>
</div>

<p>Site间的网络拓扑如下图所示，边上的值为该路径的带宽容量：</p>

<p><img src="/assets/2016-10-29-4.png" alt="Site网络拓扑图" title="Site网络拓扑图" /></p>

<p>FG的带宽函数如下图所示：</p>

<p><img src="/assets/2016-10-29-5.png" alt="FG带宽函数图" title="SFG带宽函数图" /></p>

<ol>
  <li>
    <p>Tunnel Selection</p>

    <p>利用Yen算法，选出k条最短路，此例中k取3，因此：</p>

    <p>FG1(A -&gt; B)：T1 = A -&gt; B，T2 = A -&gt; C -&gt; B，T3 = A -&gt; D -&gt; C -&gt; B</p>

    <p>FG2(A -&gt; C)：T1 = A -&gt; C，T2 = A -&gt; B -&gt; C，T3 = A -&gt; D -&gt; C</p>
  </li>
  <li>
    <p>Tunnel Group Generation</p>

    <ol>
      <li>用每个FG的最短路径进行初始化</li>
      <li>对每个FG通过相同的fair share进行带宽分配</li>
      <li>冻结包含有bottlenecked link的路径（即路径的带宽不能再分配）</li>
      <li>如果每个路径都被冻结了，或者每个FG都被满足了，则算法结束</li>
      <li>否则对于每个没有被满足的FG，选择其最短并且没被冻结的路径执行第二步</li>
    </ol>
  </li>
</ol>

<p>计算结果如下表所示：</p>

<p><img src="/assets/2016-10-29-6.png" alt="计算结果图" title="计算结果图" /></p>

<ol>
  <li>
    <p>Tunnel Group Quantization</p>

    <p>由于硬件支持的流量控制不能无限精细，需要对上一步计算出的流量进行离散化。求最优解是个整数规划问题，很难快速求解。因此论文使用了贪心算法。</p>

    <ol>
      <li>Down quantize (round) each split，我的理解是小于quanta的值为0.0，大于等于quanta的值为quanta</li>
      <li>将剩余的quanta加入一个未冻结的路径使得满足最大公平</li>
      <li>如果仍然存在quanta，则执行第二步</li>
    </ol>

    <p>从上面的计算结果得到流量分配：</p>

    <p>FG1 = 10Gbps:8.33Gbps:1.67Gbps = 0.5:0.4:0.1</p>

    <p>FG2 = 1.67Gbps:3.34Gbps = 0.3:0.7</p>

    <p>对上面的split进行Down quantize，得到：</p>

    <p>FG1 = 0.5:0.0:0.0（剩余的quanta为1.0）</p>

    <p>FG2 = 0.0:0.5（剩余的quanta为0.5）</p>

    <p>因此，例如对于FG2，剩余的0.5应该分配给那一条路径？如果分配给A-&gt;C，那么就会使得FG1的fair share降低到10*0.5=5，不满足最大公平；而将0.5分配到A-&gt;D-&gt;C则满足要求，因此最终结果为：</p>

    <p>FG1 = 1.0:0.5:0.0 = 10Gbps:10Gbps:0</p>

    <p>FG2 = 0.0:1.0 = 0:5Gbps</p>
  </li>
</ol>

